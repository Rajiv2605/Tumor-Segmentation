#
input: "data"
input_dim: 4
input_dim: 3
input_dim: 256
input_dim: 256

input: "labels"
input_dim: 4
input_dim: 1
input_dim: 256
input_dim: 256

layer {
  name: "data_sub1"
  type: "Scale"
  bottom: "data"
  top: "data_sub1"
}
layer {
  name: "data_sub2"
  type: "Interp"
  bottom: "data_sub1"
  top: "data_sub2"
  interp_param {
    shrink_factor: 2
  }
}
layer {
  name: "conv1_1_3x3_s2"
  type: "Convolution"
  bottom: "data_sub2"
  top: "conv1_1_3x3_s2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv1_1_3x3_s2/relu"
  type: "ReLU"
  bottom: "conv1_1_3x3_s2"
  top: "conv1_1_3x3_s2"
}
layer {
  name: "conv1_2_3x3"
  type: "Convolution"
  bottom: "conv1_1_3x3_s2"
  top: "conv1_2_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv1_2_3x3/relu"
  type: "ReLU"
  bottom: "conv1_2_3x3"
  top: "conv1_2_3x3"
}
layer {
  name: "conv1_3_3x3"
  type: "Convolution"
  bottom: "conv1_2_3x3"
  top: "conv1_3_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv1_3_3x3/relu"
  type: "ReLU"
  bottom: "conv1_3_3x3"
  top: "conv1_3_3x3"
}
layer {
  name: "pool1_3x3_s2"
  type: "Pooling"
  bottom: "conv1_3_3x3"
  top: "pool1_3x3_s2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv2_1_1x1_reduce"
  type: "Convolution"
  bottom: "pool1_3x3_s2"
  top: "conv2_1_1x1_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv2_1_1x1_reduce/relu"
  type: "ReLU"
  bottom: "conv2_1_1x1_reduce"
  top: "conv2_1_1x1_reduce"
}
layer {
  name: "conv2_1_3x3"
  type: "Convolution"
  bottom: "conv2_1_1x1_reduce"
  top: "conv2_1_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv2_1_3x3/relu"
  type: "ReLU"
  bottom: "conv2_1_3x3"
  top: "conv2_1_3x3"
}
layer {
  name: "conv2_1_1x1_increase"
  type: "Convolution"
  bottom: "conv2_1_3x3"
  top: "conv2_1_1x1_increase"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv2_1_1x1_proj"
  type: "Convolution"
  bottom: "pool1_3x3_s2"
  top: "conv2_1_1x1_proj"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv2_1"
  type: "Eltwise"
  bottom: "conv2_1_1x1_proj"
  bottom: "conv2_1_1x1_increase"
  top: "conv2_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_1/relu"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
#------------------------------------------------
layer {
  name: "BatchNorm1_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "BatchNorm1_1"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale1_1"
  type: "Scale"
  bottom: "BatchNorm1_1"
  top: "BatchNorm1_1"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU1_1"
  type: "ReLU"
  bottom: "BatchNorm1_1"
  top: "BatchNorm1_1"
}
layer {
  name: "Convolution1_1"
  type: "Convolution"
  bottom: "BatchNorm1_1"
  top: "Convolution1_1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    pad: 0
#    pad: 0
    kernel_size: 1
    kernel_size: 1
#    kernel_size: 1
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "BatchNorm1_2"
  type: "BatchNorm"
  bottom: "Convolution1_1"
  top: "BatchNorm1_2"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
#  param {
#    lr_mult: 0.0
#    decay_mult: 0.0
#  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale1_2"
  type: "Scale"
  bottom: "BatchNorm1_2"
  top: "BatchNorm1_2"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU1_2"
  type: "ReLU"
  bottom: "BatchNorm1_2"
  top: "BatchNorm1_2"
}
layer {
  name: "Convolution1_2"
  type: "Convolution"
  bottom: "BatchNorm1_2"
  top: "Convolution1_2"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    pad: 1
#    pad: 1
    kernel_size: 3
    kernel_size: 3
#    kernel_size: 3
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "Dropout1"
  type: "Dropout"
  bottom: "Convolution1_2"
  top: "Dropout1"
  dropout_param {
    dropout_ratio: 0.20000000298
  }
}

#-----------------------------------------------------
layer {
  name: "conv2_2"
  type: "Eltwise"
  bottom: "conv2_1"
  bottom: "Dropout1"
  top: "conv2_2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_2/relu"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}

#------------------------------------------------
layer {
  name: "BatchNorm2_1"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "BatchNorm2_1"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale2_1"
  type: "Scale"
  bottom: "BatchNorm2_1"
  top: "BatchNorm2_1"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU2_1"
  type: "ReLU"
  bottom: "BatchNorm2_1"
  top: "BatchNorm2_1"
}
layer {
  name: "Convolution2_1"
  type: "Convolution"
  bottom: "BatchNorm2_1"
  top: "Convolution2_1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    pad: 0
#    pad: 0
    kernel_size: 1
    kernel_size: 1
#    kernel_size: 1
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "BatchNorm2_2"
  type: "BatchNorm"
  bottom: "Convolution2_1"
  top: "BatchNorm2_2"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
#  param {
#    lr_mult: 0.0
#    decay_mult: 0.0
#  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale2_2"
  type: "Scale"
  bottom: "BatchNorm2_2"
  top: "BatchNorm2_2"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU2_2"
  type: "ReLU"
  bottom: "BatchNorm2_2"
  top: "BatchNorm2_2"
}
layer {
  name: "Convolution2_2"
  type: "Convolution"
  bottom: "BatchNorm2_2"
  top: "Convolution2_2"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    pad: 1
#    pad: 1
    kernel_size: 3
    kernel_size: 3
#    kernel_size: 3
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "Dropout2"
  type: "Dropout"
  bottom: "Convolution2_2"
  top: "Dropout2"
  dropout_param {
    dropout_ratio: 0.20000000298
  }
}

#-----------------------------------------------------

layer {
  name: "conv2_3"
  type: "Eltwise"
  bottom: "conv2_2"
  bottom: "Dropout2"
  top: "conv2_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_3/relu"
  type: "ReLU"
  bottom: "conv2_3"
  top: "conv2_3"
}
layer {
  name: "conv3_1_1x1_reduce"
  type: "Convolution"
  bottom: "conv2_3"
  top: "conv3_1_1x1_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv3_1_1x1_reduce/relu"
  type: "ReLU"
  bottom: "conv3_1_1x1_reduce"
  top: "conv3_1_1x1_reduce"
}
layer {
  name: "conv3_1_3x3"
  type: "Convolution"
  bottom: "conv3_1_1x1_reduce"
  top: "conv3_1_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv3_1_3x3/relu"
  type: "ReLU"
  bottom: "conv3_1_3x3"
  top: "conv3_1_3x3"
}
layer {
  name: "conv3_1_1x1_increase"
  type: "Convolution"
  bottom: "conv3_1_3x3"
  top: "conv3_1_1x1_increase"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv3_1_1x1_proj"
  type: "Convolution"
  bottom: "conv2_3"
  top: "conv3_1_1x1_proj"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv3_1"
  type: "Eltwise"
  bottom: "conv3_1_1x1_proj"
  bottom: "conv3_1_1x1_increase"
  top: "conv3_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_1/relu"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_1_sub4"
  type: "Interp"
  bottom: "conv3_1"
  top: "conv3_1_sub4"
  interp_param {
    shrink_factor: 2
  }
}

layer {
  name: "conv3_2_1x1_reduce"
  type: "Convolution"
  bottom: "conv3_1_sub4"
  top: "conv3_2_1x1_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv3_2_1x1_reduce/relu"
  type: "ReLU"
  bottom: "conv3_2_1x1_reduce"
  top: "conv3_2_1x1_reduce"
}
layer {
  name: "conv3_2_3x3"
  type: "Convolution"
  bottom: "conv3_2_1x1_reduce"
  top: "conv3_2_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv3_2_3x3/relu"
  type: "ReLU"
  bottom: "conv3_2_3x3"
  top: "conv3_2_3x3"
}
layer {
  name: "conv3_2_1x1_increase"
  type: "Convolution"
  bottom: "conv3_2_3x3"
  top: "conv3_2_1x1_increase"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}

layer {
  name: "conv3_2"
  type: "Eltwise"
  bottom: "conv3_1_sub4"
  bottom: "conv3_2_1x1_increase"
  top: "conv3_2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_2/relu"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
#------------------------------------------------
layer {
  name: "BatchNorm3_1"
  type: "BatchNorm"
  bottom: "conv3_2"
  top: "BatchNorm3_1"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale3_1"
  type: "Scale"
  bottom: "BatchNorm3_1"
  top: "BatchNorm3_1"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU3_1"
  type: "ReLU"
  bottom: "BatchNorm3_1"
  top: "BatchNorm3_1"
}
layer {
  name: "Convolution3_1"
  type: "Convolution"
  bottom: "BatchNorm3_1"
  top: "Convolution3_1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    pad: 0
#    pad: 0
    kernel_size: 1
    kernel_size: 1
#    kernel_size: 1
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "BatchNorm3_2"
  type: "BatchNorm"
  bottom: "Convolution3_1"
  top: "BatchNorm3_2"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
#  param {
#    lr_mult: 0.0
#    decay_mult: 0.0
#  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale3_2"
  type: "Scale"
  bottom: "BatchNorm3_2"
  top: "BatchNorm3_2"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU3_2"
  type: "ReLU"
  bottom: "BatchNorm3_2"
  top: "BatchNorm3_2"
}
layer {
  name: "Convolution3_2"
  type: "Convolution"
  bottom: "BatchNorm3_2"
  top: "Convolution3_2"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    pad: 1
#    pad: 1
    kernel_size: 3
    kernel_size: 3
#    kernel_size: 3
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "Dropout3"
  type: "Dropout"
  bottom: "Convolution3_2"
  top: "Dropout3"
  dropout_param {
    dropout_ratio: 0.20000000298
  }
}

#-----------------------------------------------------
layer {
  name: "conv3_3"
  type: "Eltwise"
  bottom: "conv3_2"
  bottom: "Dropout3"
  top: "conv3_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_3/relu"
  type: "ReLU"
  bottom: "conv3_3"
  top: "conv3_3"
}
#------------------------------------------------
layer {
  name: "BatchNorm4_1"
  type: "BatchNorm"
  bottom: "conv3_3"
  top: "BatchNorm4_1"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale4_1"
  type: "Scale"
  bottom: "BatchNorm4_1"
  top: "BatchNorm4_1"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU4_1"
  type: "ReLU"
  bottom: "BatchNorm4_1"
  top: "BatchNorm4_1"
}
layer {
  name: "Convolution4_1"
  type: "Convolution"
  bottom: "BatchNorm4_1"
  top: "Convolution4_1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    pad: 0
#    pad: 0
    kernel_size: 1
    kernel_size: 1
#    kernel_size: 1
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "BatchNorm4_2"
  type: "BatchNorm"
  bottom: "Convolution4_1"
  top: "BatchNorm4_2"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
#  param {
#    lr_mult: 0.0
#    decay_mult: 0.0
#  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale4_2"
  type: "Scale"
  bottom: "BatchNorm4_2"
  top: "BatchNorm4_2"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU4_2"
  type: "ReLU"
  bottom: "BatchNorm4_2"
  top: "BatchNorm4_2"
}
layer {
  name: "Convolution4_2"
  type: "Convolution"
  bottom: "BatchNorm4_2"
  top: "Convolution4_2"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    pad: 1
#    pad: 1
    kernel_size: 3
    kernel_size: 3
#    kernel_size: 3
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "Dropout4"
  type: "Dropout"
  bottom: "Convolution4_2"
  top: "Dropout4"
  dropout_param {
    dropout_ratio: 0.20000000298
  }
}

#-----------------------------------------------------
layer {
  name: "conv3_4"
  type: "Eltwise"
  bottom: "conv3_3"
  bottom: "Dropout4"
  top: "conv3_4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_4/relu"
  type: "ReLU"
  bottom: "conv3_4"
  top: "conv3_4"
}
layer {
  name: "conv4_1_1x1_reduce"
  type: "Convolution"
  bottom: "conv3_4"
  top: "conv4_1_1x1_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv4_1_1x1_reduce/relu"
  type: "ReLU"
  bottom: "conv4_1_1x1_reduce"
  top: "conv4_1_1x1_reduce"
}
layer {
  name: "conv4_1_3x3"
  type: "Convolution"
  bottom: "conv4_1_1x1_reduce"
  top: "conv4_1_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    dilation: 2
  }
}
layer {
  name: "conv4_1_3x3/relu"
  type: "ReLU"
  bottom: "conv4_1_3x3"
  top: "conv4_1_3x3"
}
layer {
  name: "conv4_1_1x1_increase"
  type: "Convolution"
  bottom: "conv4_1_3x3"
  top: "conv4_1_1x1_increase"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv4_1_1x1_proj"
  type: "Convolution"
  bottom: "conv3_4"
  top: "conv4_1_1x1_proj"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv4_1"
  type: "Eltwise"
  bottom: "conv4_1_1x1_proj"
  bottom: "conv4_1_1x1_increase"
  top: "conv4_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_1/relu"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
#------------------------------------------------
layer {
  name: "BatchNorm5_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "BatchNorm5_1"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale5_1"
  type: "Scale"
  bottom: "BatchNorm5_1"
  top: "BatchNorm5_1"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU5_1"
  type: "ReLU"
  bottom: "BatchNorm5_1"
  top: "BatchNorm5_1"
}
layer {
  name: "Convolution5_1"
  type: "Convolution"
  bottom: "BatchNorm5_1"
  top: "Convolution5_1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    pad: 0
#    pad: 0
    kernel_size: 1
    kernel_size: 1
#    kernel_size: 1
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "BatchNorm5_2"
  type: "BatchNorm"
  bottom: "Convolution5_1"
  top: "BatchNorm5_2"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
#  param {
#    lr_mult: 0.0
#    decay_mult: 0.0
#  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale5_2"
  type: "Scale"
  bottom: "BatchNorm5_2"
  top: "BatchNorm5_2"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU5_2"
  type: "ReLU"
  bottom: "BatchNorm5_2"
  top: "BatchNorm5_2"
}
layer {
  name: "Convolution5_2"
  type: "Convolution"
  bottom: "BatchNorm5_2"
  top: "Convolution5_2"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    pad: 1
#    pad: 1
    kernel_size: 3
    kernel_size: 3
#    kernel_size: 3
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "Dropout5"
  type: "Dropout"
  bottom: "Convolution5_2"
  top: "Dropout5"
  dropout_param {
    dropout_ratio: 0.20000000298
  }
}

#-----------------------------------------------------
layer {
  name: "conv4_2"
  type: "Eltwise"
  bottom: "conv4_1"
  bottom: "Dropout5"
  top: "conv4_2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_2/relu"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
#------------------------------------------------
layer {
  name: "BatchNorm6_1"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "BatchNorm6_1"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale6_1"
  type: "Scale"
  bottom: "BatchNorm6_1"
  top: "BatchNorm6_1"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU6_1"
  type: "ReLU"
  bottom: "BatchNorm6_1"
  top: "BatchNorm6_1"
}
layer {
  name: "Convolution6_1"
  type: "Convolution"
  bottom: "BatchNorm6_1"
  top: "Convolution6_1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    pad: 0
#    pad: 0
    kernel_size: 1
    kernel_size: 1
#    kernel_size: 1
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "BatchNorm6_2"
  type: "BatchNorm"
  bottom: "Convolution6_1"
  top: "BatchNorm6_2"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
#  param {
#    lr_mult: 0.0
#    decay_mult: 0.0
#  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale6_2"
  type: "Scale"
  bottom: "BatchNorm6_2"
  top: "BatchNorm6_2"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU6_2"
  type: "ReLU"
  bottom: "BatchNorm6_2"
  top: "BatchNorm6_2"
}
layer {
  name: "Convolution6_2"
  type: "Convolution"
  bottom: "BatchNorm6_2"
  top: "Convolution6_2"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    pad: 1
#    pad: 1
    kernel_size: 3
    kernel_size: 3
#    kernel_size: 3
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "Dropout6"
  type: "Dropout"
  bottom: "Convolution6_2"
  top: "Dropout6"
  dropout_param {
    dropout_ratio: 0.20000000298
  }
}

#-----------------------------------------------------
layer {
  name: "conv4_3"
  type: "Eltwise"
  bottom: "conv4_2"
  bottom: "Dropout6"
  top: "conv4_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_3/relu"
  type: "ReLU"
  bottom: "conv4_3"
  top: "conv4_3"
}
#------------------------------------------------
layer {
  name: "BatchNorm7_1"
  type: "BatchNorm"
  bottom: "conv4_3"
  top: "BatchNorm7_1"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale7_1"
  type: "Scale"
  bottom: "BatchNorm7_1"
  top: "BatchNorm7_1"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU7_1"
  type: "ReLU"
  bottom: "BatchNorm7_1"
  top: "BatchNorm7_1"
}
layer {
  name: "Convolution7_1"
  type: "Convolution"
  bottom: "BatchNorm7_1"
  top: "Convolution7_1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    pad: 0
#    pad: 0
    kernel_size: 1
    kernel_size: 1
#    kernel_size: 1
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "BatchNorm7_2"
  type: "BatchNorm"
  bottom: "Convolution7_1"
  top: "BatchNorm7_2"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
#  param {
#    lr_mult: 0.0
#    decay_mult: 0.0
#  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale7_2"
  type: "Scale"
  bottom: "BatchNorm7_2"
  top: "BatchNorm7_2"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU7_2"
  type: "ReLU"
  bottom: "BatchNorm7_2"
  top: "BatchNorm7_2"
}
layer {
  name: "Convolution7_2"
  type: "Convolution"
  bottom: "BatchNorm7_2"
  top: "Convolution7_2"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    pad: 1
#    pad: 1
    kernel_size: 3
    kernel_size: 3
#    kernel_size: 3
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "Dropout7"
  type: "Dropout"
  bottom: "Convolution7_2"
  top: "Dropout7"
  dropout_param {
    dropout_ratio: 0.20000000298
  }
}

#-----------------------------------------------------
layer {
  name: "conv4_4"
  type: "Eltwise"
  bottom: "conv4_3"
  bottom: "Dropout7"
  top: "conv4_4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_4/relu"
  type: "ReLU"
  bottom: "conv4_4"
  top: "conv4_4"
}
#------------------------------------------------
layer {
  name: "BatchNorm8_1"
  type: "BatchNorm"
  bottom: "conv4_4"
  top: "BatchNorm8_1"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale8_1"
  type: "Scale"
  bottom: "BatchNorm8_1"
  top: "BatchNorm8_1"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU8_1"
  type: "ReLU"
  bottom: "BatchNorm8_1"
  top: "BatchNorm8_1"
}
layer {
  name: "Convolution8_1"
  type: "Convolution"
  bottom: "BatchNorm8_1"
  top: "Convolution8_1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    pad: 0
#    pad: 0
    kernel_size: 1
    kernel_size: 1
#    kernel_size: 1
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "BatchNorm8_2"
  type: "BatchNorm"
  bottom: "Convolution8_1"
  top: "BatchNorm8_2"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
#  param {
#    lr_mult: 0.0
#    decay_mult: 0.0
#  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale8_2"
  type: "Scale"
  bottom: "BatchNorm8_2"
  top: "BatchNorm8_2"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU8_2"
  type: "ReLU"
  bottom: "BatchNorm8_2"
  top: "BatchNorm8_2"
}
layer {
  name: "Convolution8_2"
  type: "Convolution"
  bottom: "BatchNorm8_2"
  top: "Convolution8_2"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    pad: 1
#    pad: 1
    kernel_size: 3
    kernel_size: 3
#    kernel_size: 3
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "Dropout8"
  type: "Dropout"
  bottom: "Convolution8_2"
  top: "Dropout8"
  dropout_param {
    dropout_ratio: 0.20000000298
  }
}

#-----------------------------------------------------
layer {
  name: "conv4_5"
  type: "Eltwise"
  bottom: "conv4_4"
  bottom: "Dropout8"
  top: "conv4_5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_5/relu"
  type: "ReLU"
  bottom: "conv4_5"
  top: "conv4_5"
}
#------------------------------------------------
layer {
  name: "BatchNorm9_1"
  type: "BatchNorm"
  bottom: "conv4_5"
  top: "BatchNorm9_1"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale9_1"
  type: "Scale"
  bottom: "BatchNorm9_1"
  top: "BatchNorm9_1"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU9_1"
  type: "ReLU"
  bottom: "BatchNorm9_1"
  top: "BatchNorm9_1"
}
layer {
  name: "Convolution9_1"
  type: "Convolution"
  bottom: "BatchNorm9_1"
  top: "Convolution9_1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    pad: 0
#    pad: 0
    kernel_size: 1
    kernel_size: 1
#    kernel_size: 1
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "BatchNorm9_2"
  type: "BatchNorm"
  bottom: "Convolution9_1"
  top: "BatchNorm9_2"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
#  param {
#    lr_mult: 0.0
#    decay_mult: 0.0
#  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale9_2"
  type: "Scale"
  bottom: "BatchNorm9_2"
  top: "BatchNorm9_2"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU9_2"
  type: "ReLU"
  bottom: "BatchNorm9_2"
  top: "BatchNorm9_2"
}
layer {
  name: "Convolution9_2"
  type: "Convolution"
  bottom: "BatchNorm9_2"
  top: "Convolution9_2"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 512
    bias_term: false
    pad: 1
    pad: 1
#    pad: 1
    kernel_size: 3
    kernel_size: 3
#    kernel_size: 3
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "Dropout9"
  type: "Dropout"
  bottom: "Convolution9_2"
  top: "Dropout9"
  dropout_param {
    dropout_ratio: 0.20000000298
  }
}

#-----------------------------------------------------
layer {
  name: "conv4_6"
  type: "Eltwise"
  bottom: "conv4_5"
  bottom: "Dropout9"
  top: "conv4_6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_6/relu"
  type: "ReLU"
  bottom: "conv4_6"
  top: "conv4_6"
}
layer {
  name: "conv5_1_1x1_reduce"
  type: "Convolution"
  bottom: "conv4_6"
  top: "conv5_1_1x1_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv5_1_1x1_reduce/relu"
  type: "ReLU"
  bottom: "conv5_1_1x1_reduce"
  top: "conv5_1_1x1_reduce"
}
layer {
  name: "conv5_1_3x3"
  type: "Convolution"
  bottom: "conv5_1_1x1_reduce"
  top: "conv5_1_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    pad: 4
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    dilation: 4
  }
}
layer {
  name: "conv5_1_3x3/relu"
  type: "ReLU"
  bottom: "conv5_1_3x3"
  top: "conv5_1_3x3"
}
layer {
  name: "conv5_1_1x1_increase"
  type: "Convolution"
  bottom: "conv5_1_3x3"
  top: "conv5_1_1x1_increase"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 1024
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv5_1_1x1_proj"
  type: "Convolution"
  bottom: "conv4_6"
  top: "conv5_1_1x1_proj"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 1024
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv5_1"
  type: "Eltwise"
  bottom: "conv5_1_1x1_proj"
  bottom: "conv5_1_1x1_increase"
  top: "conv5_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv5_1/relu"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
#------------------------------------------------
layer {
  name: "BatchNorm10_1"
  type: "BatchNorm"
  bottom: "conv5_1"
  top: "BatchNorm10_1"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale10_1"
  type: "Scale"
  bottom: "BatchNorm10_1"
  top: "BatchNorm10_1"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU10_1"
  type: "ReLU"
  bottom: "BatchNorm10_1"
  top: "BatchNorm10_1"
}
layer {
  name: "Convolution10_1"
  type: "Convolution"
  bottom: "BatchNorm10_1"
  top: "Convolution10_1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    pad: 0
#    pad: 0
    kernel_size: 1
    kernel_size: 1
#    kernel_size: 1
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "BatchNorm10_2"
  type: "BatchNorm"
  bottom: "Convolution10_1"
  top: "BatchNorm10_2"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
#  param {
#    lr_mult: 0.0
#    decay_mult: 0.0
#  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale10_2"
  type: "Scale"
  bottom: "BatchNorm10_2"
  top: "BatchNorm10_2"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU10_2"
  type: "ReLU"
  bottom: "BatchNorm10_2"
  top: "BatchNorm10_2"
}
layer {
  name: "Convolution10_2"
  type: "Convolution"
  bottom: "BatchNorm10_2"
  top: "Convolution10_2"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 1
    pad: 1
#    pad: 1
    kernel_size: 3
    kernel_size: 3
#    kernel_size: 3
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "Dropout10"
  type: "Dropout"
  bottom: "Convolution10_2"
  top: "Dropout10"
  dropout_param {
    dropout_ratio: 0.20000000298
  }
}

#-----------------------------------------------------
layer {
  name: "conv5_2"
  type: "Eltwise"
  bottom: "conv5_1"
  bottom: "Dropout10"
  top: "conv5_2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv5_2/relu"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
#------------------------------------------------
layer {
  name: "BatchNorm11_1"
  type: "BatchNorm"
  bottom: "conv5_2"
  top: "BatchNorm11_1"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale11_1"
  type: "Scale"
  bottom: "BatchNorm11_1"
  top: "BatchNorm11_1"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU11_1"
  type: "ReLU"
  bottom: "BatchNorm11_1"
  top: "BatchNorm11_1"
}
layer {
  name: "Convolution11_1"
  type: "Convolution"
  bottom: "BatchNorm11_1"
  top: "Convolution11_1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    pad: 0
#    pad: 0
    kernel_size: 1
    kernel_size: 1
#    kernel_size: 1
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "BatchNorm11_2"
  type: "BatchNorm"
  bottom: "Convolution11_1"
  top: "BatchNorm11_2"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
#  param {
#    lr_mult: 0.0
#    decay_mult: 0.0
#  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale11_2"
  type: "Scale"
  bottom: "BatchNorm11_2"
  top: "BatchNorm11_2"
  scale_param {
    filler {
      value: 1.0
    }
    bias_term: true
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "ReLU11_2"
  type: "ReLU"
  bottom: "BatchNorm11_2"
  top: "BatchNorm11_2"
}
layer {
  name: "Convolution11_2"
  type: "Convolution"
  bottom: "BatchNorm11_2"
  top: "Convolution11_2"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1024
    bias_term: false
    pad: 1
    pad: 1
#    pad: 1
    kernel_size: 3
    kernel_size: 3
#    kernel_size: 3
    stride: 1
    stride: 1
#    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    axis: 1
  }
}
layer {
  name: "Dropout11"
  type: "Dropout"
  bottom: "Convolution11_2"
  top: "Dropout11"
  dropout_param {
    dropout_ratio: 0.20000000298
  }
}

#-----------------------------------------------------
layer {
  name: "conv5_3"
  type: "Eltwise"
  bottom: "conv5_2"
  bottom: "Dropout11"
  top: "conv5_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv5_3/relu"
  type: "ReLU"
  bottom: "conv5_3"
  top: "conv5_3"
}
layer {
  name: "conv5_3_pool1"
  type: "Pooling"
  bottom: "conv5_3"
  top: "conv5_3_pool1"
  pooling_param {
    pool: AVE
    kernel_h: 31
    kernel_w: 31
    stride_h: 31
    stride_w: 31
  }
}
layer {
  name: "conv5_3_pool1_interp"
  type: "Interp"
  bottom: "conv5_3_pool1"
  top: "conv5_3_pool1_interp"
  interp_param {
    height: 17
    width: 17
  }
}
layer {
  name: "conv5_3_pool2"
  type: "Pooling"
  bottom: "conv5_3"
  top: "conv5_3_pool2"
  pooling_param {
    pool: AVE
    kernel_h: 15
    kernel_w: 15
    stride_h: 15
    stride_w: 15
  }
}
layer {
  name: "conv5_3_pool2_interp"
  type: "Interp"
  bottom: "conv5_3_pool2"
  top: "conv5_3_pool2_interp"
  interp_param {
    height: 17
    width: 17
  }
}
layer {
  name: "conv5_3_pool3"
  type: "Pooling"
  bottom: "conv5_3"
  top: "conv5_3_pool3"
  pooling_param {
    pool: AVE
    kernel_h: 9
    kernel_w: 9
    stride_h: 9
    stride_w: 9
  }
}
layer {
  name: "conv5_3_pool3_interp"
  type: "Interp"
  bottom: "conv5_3_pool3"
  top: "conv5_3_pool3_interp"
  interp_param {
    height: 17
    width: 17
  }
}
layer {
  name: "conv5_3_pool6"
  type: "Pooling"
  bottom: "conv5_3"
  top: "conv5_3_pool6"
  pooling_param {
    pool: AVE
    kernel_h: 7
    kernel_w: 7
    stride_h: 7
    stride_w: 7
  }
}
layer {
  name: "conv5_3_pool6_interp"
  type: "Interp"
  bottom: "conv5_3_pool6"
  top: "conv5_3_pool6_interp"
  interp_param {
    height: 17
    width: 17
  }
}
layer {
  name: "conv5_3_sum"
  type: "Eltwise"
  bottom: "conv5_3"
  bottom: "conv5_3_pool6_interp"
  bottom: "conv5_3_pool3_interp"
  bottom: "conv5_3_pool2_interp"
  bottom: "conv5_3_pool1_interp"
  top: "conv5_3_sum"
}
layer {
  name: "conv5_4_k1"
  type: "Convolution"
  bottom: "conv5_3_sum"
  top: "conv5_4_k1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv5_4_k1/relu"
  type: "ReLU"
  bottom: "conv5_4_k1"
  top: "conv5_4_k1"
}
layer {
  name: "conv5_4_interp"
  type: "Interp"
  bottom: "conv5_4_k1"
  top: "conv5_4_interp"
  interp_param {
    height: 33
    width: 33
  }
}
layer {
  name: "conv_sub4"
  type: "Convolution"
  bottom: "conv5_4_interp"
  top: "conv_sub4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    dilation: 2
  }
}
layer {
  name: "conv3_1_sub2_proj"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_1_sub2_proj"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "sub24_sum"
  type: "Eltwise"
  bottom: "conv3_1_sub2_proj"
  bottom: "conv_sub4"
  top: "sub24_sum"
}
layer {
  name: "sub24_sum/relu"
  type: "ReLU"
  bottom: "sub24_sum"
  top: "sub24_sum"
}
layer {
  name: "sub24_sum_interp"
  type: "Interp"
  bottom: "sub24_sum"
  top: "sub24_sum_interp"
  interp_param {
    height: 32
    width: 32
  }
}
layer {
  name: "conv_sub2"
  type: "Convolution"
  bottom: "sub24_sum_interp"
  top: "conv_sub2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    dilation: 2
  }
}
layer {
  name: "conv1_sub1"
  type: "Convolution"
  bottom: "data_sub1"
  top: "conv1_sub1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv1_sub1/relu"
  type: "ReLU"
  bottom: "conv1_sub1"
  top: "conv1_sub1"
}
layer {
  name: "conv2_sub1"
  type: "Convolution"
  bottom: "conv1_sub1"
  top: "conv2_sub1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv2_sub1/relu"
  type: "ReLU"
  bottom: "conv2_sub1"
  top: "conv2_sub1"
}
layer {
  name: "conv3_sub1"
  type: "Convolution"
  bottom: "conv2_sub1"
  top: "conv3_sub1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv3_sub1/relu"
  type: "ReLU"
  bottom: "conv3_sub1"
  top: "conv3_sub1"
}
layer {
  name: "conv3_sub1_proj"
  type: "Convolution"
  bottom: "conv3_sub1"
  top: "conv3_sub1_proj"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "sub12_sum"
  type: "Eltwise"
  bottom: "conv3_sub1_proj"
  bottom: "conv_sub2"
  top: "sub12_sum"
}
layer {
  name: "sub12_sum/relu"
  type: "ReLU"
  bottom: "sub12_sum"
  top: "sub12_sum"
}
layer {
  name: "sub12_sum_interp"
  type: "Interp"
  bottom: "sub12_sum"
  top: "sub12_sum_interp"
  interp_param {
    zoom_factor: 2
  }
}
layer {
  name: "conv6_cls_new"
  type: "Convolution"
  bottom: "sub12_sum_interp"
  top: "conv6_cls_new"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  convolution_param {
    num_output: 2
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv6_interp"
  type: "Interp"
  bottom: "conv6_cls_new"
  top: "conv6_interp"
  interp_param {
    height: 256
    width: 256
  }
}

layer {
  name: "loss_cls"
  type: "SoftmaxWithLoss"
  bottom: "conv6_interp"
  bottom: "labels"
  top: "loss_cls"
}
